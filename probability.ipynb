{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "685bcb47-faaa-42d9-8c33-9f59452e37fb",
   "metadata": {},
   "source": [
    "# Probability\n",
    "\n",
    "## Information\n",
    "\n",
    "Information theory was developed to quantify the informational value of a communicated message.\n",
    "Specifically, we say that the value is determined by how surprising the content of the message is.\n",
    "If the message is very surprising, then we say it carries high information.\n",
    "Formally, we define the information content of an event $E$ as\n",
    "\n",
    "$$I(E) = - \\log P(E)$$\n",
    "\n",
    "Typically, the log is base 2 to capture how messages are encoded in bits within a computer. \n",
    "You can verify yourself that this function is such that $E$ has a lot of information if it has low probability, and very little information if it has high probability.\n",
    "\n",
    "## Entropy\n",
    "\n",
    "The entropy of a random variable quantifies the average level of uncertainty (or information) associated with the variableâ€™s potential states.\n",
    "In other words, it is the expected value of the variable.\n",
    "Suppose we have a discrete random variable $X$, which takes values in the set $\\mathcal{X}$.\n",
    "Its entropy is defined as:\n",
    "\n",
    "$$H(X) := \\mathbb{E} [I(X)] = -\\sum_{x \\in \\mathcal{X}} p(x) \\log p(x)$$\n",
    "\n",
    "\n",
    "## Conditional Entropy\n",
    "\n",
    "We can also define the conditional entropy of two variables, where $X$ takes values from the set $\\mathcal{X}$ and $Y$ takes values from the set $\\mathcal{Y}$.\n",
    "The derivation of this quantity is similar to that of entropy.\n",
    "Specifically, we consider the information content of an event $Y=y$ given $X=x$ as\n",
    "\n",
    "$$I(Y=y|X=x) = - \\log P(Y=y|X=x)$$\n",
    "\n",
    "The conditional entropy is defined as the expected value of the information of the conditional distribution:\n",
    "\n",
    "$$H(Y|X) = \\mathbb{E} [ I(Y|X) ] = - \\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} p(x, y) \\log \\frac{p(x, y)}{p(x)}$$\n",
    "\n",
    "where we have rewritten the last log term using Bayes Rule.\n",
    "\n",
    "\n",
    "## Information gain in decision tree splitting\n",
    "\n",
    "Now that we have defined entropy and conditional entropy, we can defined information gain when splitting a node in the decision tree.\n",
    "Given a set of training examples $E$\n",
    "Let's define $E_{y=y_i}$ to be the subset of the dataset $E$ where the the variable $y$ takes the value $y_i$.\n",
    "In other words, $E_{y=y_i} = \\{(x, y) \\in E \\ \\mathrm{where}\\ y=y_i \\}$.\n",
    "The probability of drawing an example where $y = y_i$ is then:\n",
    "\n",
    "$$p(Y=y_i) = p(y_i) = \\frac{|E_{y=y_i}|}{|E|}$$\n",
    "\n",
    "The entropy for the dataset $E$ is:\n",
    "\n",
    "$$H(E) = -\\sum_{y_i \\in Y} p(y_i) \\log p(y_i)$$\n",
    "\n",
    "When we split the dataset based on some variable $x$, we define the information gain as the change in entropy after the split.\n",
    "Let's say there are only two possible values for $\\mathcal{X} = [0, 1]$.\n",
    "The conditional entropy $H(E|x)$ is:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(E|x) &=& - \\left[ p(x=0) \\sum_{y \\in \\mathcal{Y}} p(y|x=0) \\log p(y|x=0) + p(x=1) \\sum_{y \\in \\mathcal{Y}} p(y|x=1) \\log p(y|x=1) \\right]\n",
    "\\\\\n",
    "&=& - p(x=0) \\sum_{y \\in \\mathcal{Y}} p(y|x=0) \\log p(y|x=0) - p(x=1) \\sum_{y \\in \\mathcal{Y}} p(y|x=1) \\log p(y|x=1)\n",
    "\\\\\n",
    "&=& - \\frac{|E_{x=0}|}{|E|} \\sum_{y \\in \\mathcal{Y}} p(y|x=0) \\log p(y|x=0) - \\frac{|E_{x=1}|}{|E|} \\sum_{y \\in \\mathcal{Y}} p(y|x=1) \\log p(y|x=1)\n",
    "\\\\\n",
    "&=& \\frac{|E_{x=0}|}{|E|} H(E_{x=0}) + \\frac{|E_{x=1}|}{|E|} H(E_{x=1})\n",
    "\\end{eqnarray}\n",
    "\n",
    "In general, we have:\n",
    "$$H(E|x) = \\sum_{x_i \\in \\mathcal{X}} \\frac{|E_{x=x_i}|}{|E|} H(E_{x=x_i})$$\n",
    "\n",
    "We can consequently simplify information gain as:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{IG}(E, x) &=& H(E) - H(E|x)\n",
    "\\\\\n",
    "&=& H(E) - \\sum_{x_i \\in \\mathcal{X}} \\frac{|E_{x=x_i}|}{|E|} H(E_{x=x_i})\n",
    "\\end{eqnarray}\n",
    "\n",
    "In practice, what this means is that we can define information gain from a split using the entropy of the \"current node\" before the split and a weighted sum of the would-be entropy of the \"child nodes\" after the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e2a40-f69c-49b9-932a-dce130106159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
